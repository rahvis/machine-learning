{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Naive Bayes Classifier: An Overview**\n",
        "\n",
        "The **Naive Bayes classifier** is a probabilistic classification algorithm based on **Bayes' Theorem**, with the **naive assumption** that the features are conditionally independent given the class. Despite this simplifying assumption, Naive Bayes works remarkably well for many real-world applications, particularly when dealing with large datasets, such as in text classification or spam filtering.\n",
        "\n",
        "Bayes' Theorem states:\n",
        "\\[\n",
        "P(C|X) = \\frac{P(X|C) P(C)}{P(X)}\n",
        "\\]\n",
        "Where:\n",
        "- \\( P(C|X) \\) is the **posterior probability** of the class \\( C \\) given the features \\( X \\).\n",
        "- \\( P(X|C) \\) is the **likelihood** of the features \\( X \\) given the class \\( C \\).\n",
        "- \\( P(C) \\) is the **prior probability** of the class \\( C \\).\n",
        "- \\( P(X) \\) is the **evidence** or the probability of the features \\( X \\).\n",
        "\n",
        "### **Steps for Naive Bayes Classifier**\n",
        "1. **Assume Conditional Independence**:\n",
        "   We assume that the features \\( X_1, X_2, \\dots, X_n \\) are conditionally independent given the class label \\( C \\). This is the \"naive\" assumption.\n",
        "   \n",
        "   So, we can write:\n",
        "   \\[\n",
        "   P(X|C) = P(X_1|C) \\cdot P(X_2|C) \\cdot \\dots \\cdot P(X_n|C)\n",
        "   \\]\n",
        "\n",
        "2. **Calculate the Prior Probability \\( P(C) \\)**:\n",
        "   The prior is the probability of each class in the dataset.\n",
        "\n",
        "3. **Calculate the Likelihood \\( P(X|C) \\)**:\n",
        "   For each feature \\( X_i \\), calculate the conditional probability \\( P(X_i|C) \\).\n",
        "\n",
        "4. **Calculate the Posterior \\( P(C|X) \\)**:\n",
        "   Using Bayes' theorem, we calculate the posterior probability of each class \\( C \\) given the input features \\( X \\).\n",
        "\n",
        "5. **Make the Prediction**:\n",
        "   The class with the highest posterior probability is predicted.\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Let's consider a dataset of weather conditions and whether or not someone will play tennis based on that weather:\n",
        "- **Features**: Outlook (Sunny, Overcast, Rain), Temperature (Hot, Mild, Cool), Humidity (High, Low)\n",
        "- **Class Label**: PlayTennis (Yes, No)\n",
        "\n",
        "Here's a simple dataset:\n",
        "\n",
        "| Outlook   | Temperature | Humidity | PlayTennis |\n",
        "|-----------|-------------|----------|------------|\n",
        "| Sunny     | Hot         | High     | No         |\n",
        "| Sunny     | Hot         | High     | No         |\n",
        "| Overcast  | Hot         | High     | Yes        |\n",
        "| Rain      | Mild        | High     | Yes        |\n",
        "| Rain      | Cool        | Low      | Yes        |\n",
        "| Rain      | Cool        | Low      | No         |\n",
        "| Overcast  | Cool        | Low      | Yes        |\n",
        "| Sunny     | Mild        | High     | No         |\n",
        "| Sunny     | Cool        | Low      | Yes        |\n",
        "| Rain      | Mild        | Low      | Yes        |\n",
        "\n",
        "### **Step-by-Step Approach**:\n",
        "\n",
        "1. **Calculate Prior Probabilities**: The probability of \"Yes\" and \"No\" based on the classes in the training set.\n",
        "   \n",
        "2. **Calculate Likelihood for Each Feature**: The likelihood for each feature (e.g., Outlook = Sunny) given the class (e.g., PlayTennis = Yes).\n",
        "\n",
        "3. **Apply Bayes' Theorem**: For a given test instance, apply Bayes' theorem to compute the posterior probability for each class (\"Yes\" and \"No\").\n",
        "\n",
        "4. **Prediction**: Choose the class with the highest posterior probability.\n",
        "\n",
        "### **Naive Bayes Classifier Code from Scratch**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sample Data (features + class)\n",
        "data = [\n",
        "    ['Sunny', 'Hot', 'High', 'No'],\n",
        "    ['Sunny', 'Hot', 'High', 'No'],\n",
        "    ['Overcast', 'Hot', 'High', 'Yes'],\n",
        "    ['Rain', 'Mild', 'High', 'Yes'],\n",
        "    ['Rain', 'Cool', 'Low', 'Yes'],\n",
        "    ['Rain', 'Cool', 'Low', 'No'],\n",
        "    ['Overcast', 'Cool', 'Low', 'Yes'],\n",
        "    ['Sunny', 'Mild', 'High', 'No'],\n",
        "    ['Sunny', 'Cool', 'Low', 'Yes'],\n",
        "    ['Rain', 'Mild', 'Low', 'Yes']\n",
        "]\n",
        "\n",
        "# Convert to numpy array\n",
        "data = np.array(data)\n",
        "\n",
        "# Features and Target (Class)\n",
        "X = data[:, :-1]  # All columns except the last one\n",
        "y = data[:, -1]   # Last column (target)\n",
        "\n",
        "# Step 1: Calculate prior probabilities P(C)\n",
        "def calculate_prior(y):\n",
        "    class_counts = defaultdict(int)\n",
        "    for label in y:\n",
        "        class_counts[label] += 1\n",
        "    total_samples = len(y)\n",
        "    prior = {label: count / total_samples for label, count in class_counts.items()}\n",
        "    return prior\n",
        "\n",
        "# Step 2: Calculate likelihoods P(X_i | C)\n",
        "def calculate_likelihood(X, y):\n",
        "    feature_likelihood = defaultdict(lambda: defaultdict(int))\n",
        "    feature_count = defaultdict(int)\n",
        "    \n",
        "    for i in range(len(X)):\n",
        "        label = y[i]\n",
        "        for j in range(X.shape[1]):\n",
        "            feature_likelihood[label][(X[i, j], j)] += 1\n",
        "            feature_count[label] += 1\n",
        "    \n",
        "    # Convert to probabilities\n",
        "    for label in feature_likelihood:\n",
        "        for feature, count in feature_likelihood[label].items():\n",
        "            feature_likelihood[label][feature] = count / feature_count[label]\n",
        "    \n",
        "    return feature_likelihood, feature_count\n",
        "\n",
        "# Step 3: Predict class labels\n",
        "def predict(X, prior, feature_likelihood, feature_count):\n",
        "    predictions = []\n",
        "    \n",
        "    for x in X:\n",
        "        class_probabilities = {}\n",
        "        \n",
        "        for label in prior:\n",
        "            probability = np.log(prior[label])  # log(P(C))\n",
        "            \n",
        "            # Multiply by feature likelihoods\n",
        "            for j in range(len(x)):\n",
        "                feature = (x[j], j)\n",
        "                if feature in feature_likelihood[label]:\n",
        "                    probability += np.log(feature_likelihood[label][feature])\n",
        "                else:\n",
        "                    # If feature not found, use smoothing (Assuming probability 1/(count+1))\n",
        "                    probability += np.log(1 / (feature_count[label] + len(np.unique(X[:, j]))))\n",
        "            \n",
        "            class_probabilities[label] = probability\n",
        "        \n",
        "        # Choose the class with the highest probability\n",
        "        predicted_class = max(class_probabilities, key=class_probabilities.get)\n",
        "        predictions.append(predicted_class)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "# Training phase\n",
        "prior = calculate_prior(y)\n",
        "feature_likelihood, feature_count = calculate_likelihood(X, y)\n",
        "\n",
        "# Prediction phase (Example test)\n",
        "X_test = np.array([\n",
        "    ['Sunny', 'Cool', 'High'],  # Predict if it will play tennis\n",
        "    ['Overcast', 'Mild', 'Low']\n",
        "])\n",
        "\n",
        "# Predict the class labels for the test samples\n",
        "predictions = predict(X_test, prior, feature_likelihood, feature_count)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n",
        "```\n",
        "\n",
        "### **Explanation of the Code**\n",
        "\n",
        "1. **Prior Probability**:\n",
        "   - We calculate the prior probability \\( P(C) \\) for each class. This is the proportion of each class in the dataset.\n",
        "   \n",
        "   ```python\n",
        "   def calculate_prior(y):\n",
        "       class_counts = defaultdict(int)\n",
        "       for label in y:\n",
        "           class_counts[label] += 1\n",
        "       total_samples = len(y)\n",
        "       prior = {label: count / total_samples for label, count in class_counts.items()}\n",
        "       return prior\n",
        "   ```\n",
        "\n",
        "2. **Likelihood**:\n",
        "   - We calculate the likelihood \\( P(X_i | C) \\), the probability of a feature \\( X_i \\) given a class \\( C \\). This is done by counting the occurrences of each feature value for each class and dividing by the total occurrences of that class.\n",
        "   \n",
        "   ```python\n",
        "   def calculate_likelihood(X, y):\n",
        "       feature_likelihood = defaultdict(lambda: defaultdict(int))\n",
        "       feature_count = defaultdict(int)\n",
        "       \n",
        "       for i in range(len(X)):\n",
        "           label = y[i]\n",
        "           for j in range(X.shape[1]):\n",
        "               feature_likelihood[label][(X[i, j], j)] += 1\n",
        "               feature_count[label] += 1\n",
        "       \n",
        "       for label in feature_likelihood:\n",
        "           for feature, count in feature_likelihood[label].items():\n",
        "               feature_likelihood[label][feature] = count / feature_count[label]\n",
        "       \n",
        "       return feature_likelihood, feature_count\n",
        "   ```\n",
        "\n",
        "3. **Prediction**:\n",
        "   - We use the **logarithm** of the probabilities to avoid dealing with very small numbers. For each test instance, we compute the class probability using Bayes' Theorem, then predict the class with the highest posterior probability.\n",
        "   \n",
        "   ```python\n",
        "   def predict(X, prior, feature_likelihood, feature_count):\n",
        "       predictions = []\n",
        "       \n",
        "       for x in X:\n",
        "           class_probabilities = {}\n",
        "           \n",
        "           for label in prior:\n",
        "               probability = np.log(prior[label])\n",
        "               \n",
        "               for j in range(len(x)):\n",
        "                   feature = (x[j], j)\n",
        "                   if feature in feature_likelihood[label]:\n",
        "                       probability += np.log(feature_likelihood[label][feature])\n",
        "                   else:\n",
        "                       probability += np.log(1 / (feature_count[label] + len(np.unique(X[:, j]))))\n",
        "               \n",
        "               class_probabilities[label] = probability\n",
        "           \n",
        "           predicted_class = max(class_probabilities, key=class_probabilities.get)\n",
        "           predictions.append(predicted_class)\n",
        "       \n",
        "       return predictions\n",
        "   ```\n",
        "\n",
        "### **Example Output**:\n",
        "\n",
        "```python\n",
        "Predictions: ['No', 'Yes']\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- The test data `['Sunny', 'Cool', 'High']` is predicted as \"No\".\n",
        "- The test data `['Overcast', 'Mild', 'Low']` is predicted as \"Yes\".\n",
        "\n"
      ],
      "metadata": {
        "id": "1zz7Y5ubvSOl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-u-UQunvTM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}